{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import TrigramAssocMeasures, BigramAssocMeasures\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loads data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport_name</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>author_country</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>experience_airport</th>\n",
       "      <th>date_visit</th>\n",
       "      <th>type_traveller</th>\n",
       "      <th>overall_rating</th>\n",
       "      <th>queuing_rating</th>\n",
       "      <th>terminal_cleanliness_rating</th>\n",
       "      <th>terminal_seating_rating</th>\n",
       "      <th>terminal_signs_rating</th>\n",
       "      <th>food_beverages_rating</th>\n",
       "      <th>airport_shopping_rating</th>\n",
       "      <th>wifi_connectivity_rating</th>\n",
       "      <th>airport_staff_rating</th>\n",
       "      <th>recommended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>bilbao-airport</td>\n",
       "      <td>/airport-reviews/bilbao-airport</td>\n",
       "      <td>Bilbao Airport customer review</td>\n",
       "      <td>Borja Diez</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9/8/2004</td>\n",
       "      <td>The airport designed by Santiago Calatrava it'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17220</th>\n",
       "      <td>vienna-airport</td>\n",
       "      <td>/airport-reviews/vienna-airport</td>\n",
       "      <td>Vienna Airport customer review</td>\n",
       "      <td>Simon Smith</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5/29/2008</td>\n",
       "      <td>I used this airport in transit from LHR to Sof...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         airport_name                             link  \\\n",
       "1971   bilbao-airport  /airport-reviews/bilbao-airport   \n",
       "17220  vienna-airport  /airport-reviews/vienna-airport   \n",
       "\n",
       "                                title       author author_country       date  \\\n",
       "1971   Bilbao Airport customer review   Borja Diez            NaN   9/8/2004   \n",
       "17220  Vienna Airport customer review  Simon Smith            NaN  5/29/2008   \n",
       "\n",
       "                                                 content experience_airport  \\\n",
       "1971   The airport designed by Santiago Calatrava it'...                NaN   \n",
       "17220  I used this airport in transit from LHR to Sof...                NaN   \n",
       "\n",
       "      date_visit type_traveller  overall_rating  queuing_rating  \\\n",
       "1971         NaN            NaN             NaN             NaN   \n",
       "17220        NaN            NaN             2.0             NaN   \n",
       "\n",
       "       terminal_cleanliness_rating  terminal_seating_rating  \\\n",
       "1971                           NaN                      NaN   \n",
       "17220                          NaN                      NaN   \n",
       "\n",
       "       terminal_signs_rating  food_beverages_rating  airport_shopping_rating  \\\n",
       "1971                     NaN                    NaN                      NaN   \n",
       "17220                    NaN                    NaN                      NaN   \n",
       "\n",
       "       wifi_connectivity_rating  airport_staff_rating  recommended  \n",
       "1971                        NaN                   NaN            0  \n",
       "17220                       NaN                   NaN            0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewsdf = pd.read_csv(\"airportquality.csv\")\n",
    "reviewsdf.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "reviews = [row[\"content\"] for index, row in reviewsdf.iterrows()]\n",
    "ratings = [row[\"terminal_cleanliness_rating\"] for index, row in reviewsdf.iterrows()]\n",
    "recommended = [row[\"recommended\"] for index, row in reviewsdf.iterrows()]\n",
    "print(recommended[2:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Set up functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airport', 'moment', 'expand', 'airport', 'lot', 'build', 'go', 'departur']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "class Cleaner:\n",
    "    def __init__(self, in_bigrams = [], in_trigrams= []):\n",
    "        self.bigrams = in_bigrams\n",
    "        self.trigrams = in_trigrams\n",
    "        \n",
    "    \n",
    "    def stem_tokens(self, text):\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        tokens = [word for word in tokenizer.tokenize(text.lower())]\n",
    "\n",
    "        \n",
    "        stemmed_tokens = [term for term in tokens if term not in stopwords]\n",
    "        \n",
    "        if len(self.bigrams) > 0:\n",
    "            doc_bigrams = list(nltk.bigrams (stemmed_tokens))\n",
    "            for term in doc_bigrams:\n",
    "                if term in bigrams:\n",
    "                    stemmed_tokens.append(\"_\".join(term))\n",
    "        \n",
    "        if len(self.trigrams) > 0:\n",
    "            doc_trigrams = list(nltk.trigrams (stemmed_tokens))\n",
    "            for term in doc_trigrams:\n",
    "                if term in trigrams:\n",
    "                    stemmed_tokens.append(\"_\".join(term))\n",
    "        \n",
    "        stemmed_tokens = [stemmer.stem(word) for word in stemmed_tokens \n",
    "                          if re.search('[a-zA-Z]', word)]\n",
    "        \n",
    "        return stemmed_tokens\n",
    "\n",
    "cl = Cleaner()\n",
    "print(cl.stem_tokens(reviews[1])[2:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding ngram and prepare text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('duti', 'free'), ('passport', 'control'), ('x', 'ray'), ('car', 'park'), ('baggag', 'claim')]\n",
      "[('duti', 'free', 'shop'), ('duti', 'free', 'store'), ('duti', 'free', 'area'), ('duti', 'free', 'expens'), ('small', 'duti', 'free')]\n"
     ]
    }
   ],
   "source": [
    "text_cleaner =  Cleaner()\n",
    "texts = [text_cleaner.stem_tokens(text) for text in reviews]\n",
    "\n",
    "bigramfinder = BigramCollocationFinder.from_documents(texts)\n",
    "bigramfinder.apply_freq_filter (30)\n",
    "bigrams = bigramfinder.nbest(BigramAssocMeasures.likelihood_ratio,200)\n",
    "bigrams = [(x,y) for x,y in bigrams if x!=y]\n",
    "print(bigrams[0:5])\n",
    "trigramfinder = TrigramCollocationFinder.from_documents(texts)\n",
    "trigramfinder.apply_freq_filter (30)\n",
    "trigrams = trigramfinder.nbest(TrigramAssocMeasures.likelihood_ratio,200)\n",
    "trigrams = [(x,y,z) for x,y,z in trigrams if x!=y or x!=z or y!=z]\n",
    "print(trigrams[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_cleaner =  Cleaner(bigrams, trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf - Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define transformer \n",
    "tfidf = TfidfVectorizer(max_df=0.5, max_features=200000, \n",
    "                        min_df = 0.01, stop_words=\"english\", \n",
    "                        use_idf = True, tokenizer=text_cleaner.stem_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_reviews = tfidf.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "def cosine_distance (X, Y=None, Y_norm_squared=None, squared=False):\n",
    "    return cosine_distances(X, Y)\n",
    "\n",
    "from sklearn.cluster import k_means_\n",
    "k_means_.euclidean_distances = cosine_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=6, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "number = 6\n",
    "km = KMeans(n_clusters=number)\n",
    "km.fit(tfidf_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>rating</th>\n",
       "      <th>cluster</th>\n",
       "      <th>recommended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>San Juan airport is the best airport in the ca...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Long queues at immigration and security check ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  rating  cluster  \\\n",
       "1  San Juan airport is the best airport in the ca...     NaN        1   \n",
       "5  Long queues at immigration and security check ...     3.0        5   \n",
       "\n",
       "   recommended  \n",
       "1            0  \n",
       "5            0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap_reviews = {\"content\":reviews, \"rating\" : ratings, \"cluster\" : clusters, \"recommended\" : recommended}\n",
    "frame = pd.DataFrame(ap_reviews, index = [clusters], \n",
    "                     columns=['content' , 'rating', 'cluster', 'recommended'])\n",
    "frame['cluster'].value_counts()\n",
    "frame.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster\n",
      "0    0.159881\n",
      "1    0.274449\n",
      "2    0.195105\n",
      "3    0.348183\n",
      "4    0.187166\n",
      "5    0.158443\n",
      "Name: recommended, dtype: float64\n",
      "0    5035\n",
      "5    4033\n",
      "3    4018\n",
      "1    2270\n",
      "2    1430\n",
      "4     935\n",
      "Name: cluster, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "grouped = frame['recommended'].groupby(frame['cluster'])\n",
    "print(grouped.mean())\n",
    "print(frame['cluster'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts_by_cluster = []\n",
    "for i in range(0,number):\n",
    "    texts_by_cluster.append([])\n",
    "\n",
    "for index, row in frame.iterrows():\n",
    "    for i in range (0, number):\n",
    "        if i == row[\"cluster\"]:\n",
    "            test = row[\"content\"]\n",
    "            tokens = text_cleaner.stem_tokens(test)\n",
    "            texts_by_cluster[i].append(tokens)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame.to_csv(\"Cluster raing and rec.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_in_cat(texts):\n",
    "    words_inclass = nltk.TextCollection(texts)\n",
    "    WordDist = nltk.FreqDist(words_inclass)\n",
    "    return (WordDist.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords by word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:[('airport', 8751), ('staff', 2770), ('secur', 2466), ('time', 2284), ('one', 1890), ('use', 1836), ('get', 1775), ('travel', 1655)]\n",
      "\n",
      "1:[('termin', 6401), ('airport', 3423), ('flight', 1539), ('arriv', 1388), ('secur', 1332), ('time', 1163), ('one', 1105), ('check', 1100)]\n",
      "\n",
      "2:[('passport', 2406), ('control', 1977), ('airport', 1924), ('passport_control', 1696), ('arriv', 1241), ('queue', 1223), ('check', 1070), ('flight', 1057)]\n",
      "\n",
      "3:[('airport', 6998), ('shop', 2775), ('check', 2146), ('good', 2130), ('secur', 2106), ('area', 1929), ('free', 1879), ('departur', 1674)]\n",
      "\n",
      "4:[('airport', 1735), ('car', 1520), ('park', 1259), ('termin', 620), ('get', 584), ('arriv', 553), ('minut', 510), ('time', 503)]\n",
      "\n",
      "5:[('airport', 6202), ('flight', 5764), ('arriv', 4253), ('check', 4133), ('time', 3614), ('secur', 3599), ('hour', 2891), ('immigr', 2886)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tp = 0 \n",
    "for cl in texts_by_cluster:\n",
    "    print(str(tp) + \":\" + str(count_in_cat(cl)[0:8]))\n",
    "    print(\"\")\n",
    "    tp += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword by relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from operator import itemgetter\n",
    "# I now create a function to compute relevancy of a word given a topic.\n",
    "# Formula used is from https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf\n",
    "# now the actual funtion definiton AllW: a list of all tokens, WinT: A list of all tokens in topic, l: constant\n",
    "def computeRelevance (AllW , WinT, l):\n",
    "    #First we count word in AllW\n",
    "    AllCol = nltk.TextCollection(AllW)\n",
    "    AllDist = nltk.FreqDist(AllCol)\n",
    "    #Next we do the same for the topic\n",
    "    TCol = nltk.TextCollection(WinT)\n",
    "    TDist = nltk.FreqDist(TCol)\n",
    "    \n",
    "    wordRelv = []\n",
    "    for w, c in TDist.items():\n",
    "        relevance = math.exp((l * math.log(c/len(TCol)) + ((1-l) * math.log((c/len(TCol))/(AllDist[w]/len(AllCol))))))\n",
    "        wordRelv.append((w,relevance))\n",
    "\n",
    "    return sorted(wordRelv, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:[('airport', 0.14582586726181873), ('staff', 0.07955501060934923), ('secur', 0.060820750333789586), ('travel', 0.059214093267843444), ('time', 0.05849654062816647), ('use', 0.056712869016471684), ('one', 0.05315402207338747), ('get', 0.04895570308155398), ('go', 0.04568842556848302), ('year', 0.04427868376136235)]\n",
      "\n",
      "1:[('termin', 0.264357898069231), ('airport', 0.09917191804106196), ('new', 0.07123276141982192), ('intern', 0.06964515976635742), ('domest', 0.06441081018702124), ('flight', 0.06387705808255645), ('arriv', 0.06088276669782224), ('secur', 0.0571173244780639), ('use', 0.056980893463789935), ('one', 0.05403081715749039)]\n",
      "\n",
      "2:[('passport', 0.23600464721988956), ('control', 0.21876116758297146), ('passport_control', 0.2157978603143806), ('queue', 0.09760589868737307), ('airport', 0.08452982460030525), ('arriv', 0.08254671248589221), ('check', 0.07181008089945155), ('flight', 0.06652798350927772), ('get', 0.06420866706132745), ('secur', 0.05936869393781771)]\n",
      "\n",
      "3:[('airport', 0.1444042786489037), ('shop', 0.11118892314305727), ('good', 0.08985116918579603), ('free', 0.08845816906299489), ('clean', 0.08158804321729822), ('area', 0.07441776174414426), ('food', 0.0736098976095065), ('small', 0.06974553945712708), ('check', 0.06764451372134962), ('duti', 0.06630445117860731)]\n",
      "\n",
      "4:[('car', 0.29112820319492494), ('park', 0.2388168500016136), ('car_park', 0.15374115888925355), ('rental', 0.12445396355266133), ('airport', 0.1176431286503134), ('drop', 0.09729810701985168), ('rental_car', 0.08488068922183505), ('charg', 0.06881217265749488), ('pick', 0.06853848126064693), ('minut', 0.06650701498098573)]\n",
      "\n",
      "5:[('flight', 0.1144062435638882), ('arriv', 0.08921127867324058), ('check', 0.08747086572439931), ('airport', 0.08592767150594971), ('immigr', 0.07717368344341007), ('time', 0.0769567342187342), ('hour', 0.07688698789316414), ('secur', 0.07380145861565844), ('line', 0.07191374086419006), ('wait', 0.07100630051774727)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alltext = []\n",
    "for cl in texts_by_cluster:\n",
    "    alltext = alltext + cl \n",
    "\n",
    "tp = 0 \n",
    "for cl in texts_by_cluster:\n",
    "    print(str(tp) + \":\" + str(computeRelevance(alltext, cl, 0.6)[0:10]))\n",
    "    print(\"\")\n",
    "    tp +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic4 = computeRelevance(alltext, texts_by_cluster[3], 0.6)\n",
    "word4 =[]\n",
    "rel4 = []\n",
    "for x, y in topic4:\n",
    "    word4.append(x)\n",
    "    rel4.append(y)\n",
    "\n",
    "topic5w = {\"word\": word4, \"relevance\" : rel4}\n",
    "topic5Words = pd.DataFrame(topic4, \n",
    "                     columns=['word' , 'relevance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic5Words.to_csv(\"topic4words.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic0 = computeRelevance(alltext, texts_by_cluster[0], 0.6)\n",
    "word0 =[]\n",
    "rel0 = []\n",
    "for x, y in topic0:\n",
    "    word0.append(x)\n",
    "    rel0.append(y)\n",
    "\n",
    "topic0w = {\"word\": word0, \"relevance\" : rel0}\n",
    "topic0Words = pd.DataFrame(topic0w, \n",
    "                     columns=['word' , 'relevance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic0Words.to_csv(\"topic0words.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Predicting  in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frameShuffel = shuffle(frame)\n",
    "frame.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cReviews = [row[\"content\"] for index, row in frameShuffel.iterrows()]\n",
    "cCluster = [row[\"cluster\"] for index, row in frameShuffel.iterrows()]\n",
    "cRecommend = [row[\"recommended\"] for index, row in frameShuffel.iterrows()]\n",
    "print(len(cReviews))\n",
    "print(len(cCluster))\n",
    "print(len(cRecommend))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "review_by_Cluster = []\n",
    "for i in range(0,10):\n",
    "    review_by_Cluster.append([])\n",
    "recommend_by_Cluster = []\n",
    "for i in range(0,10):\n",
    "    recommend_by_Cluster.append([])\n",
    "    \n",
    "for x in range(0, 15000):\n",
    "    review_by_Cluster[cCluster[x]].append(cReviews[x])\n",
    "    recommend_by_Cluster[cCluster[x]].append(cRecommend[x])\n",
    "\n",
    "print(review_by_Cluster[1][1])\n",
    "print(recommend_by_Cluster[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_df=0.5, min_df=0.01, tokenizer=text_cleaner.stem_tokens)\n",
    "vectorizer.fit(cReviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## learning\n",
    "networks = []\n",
    "for i in range(0,7):\n",
    "    networks.append(MLPClassifier(hidden_layer_sizes=(80, 80), activation=\"relu\", early_stopping = True,\n",
    "                            validation_fraction = 0.1, max_iter=400, learning_rate = \"invscaling\", tol = -0.01))\n",
    "    vRv = vectorizer.transform(review_by_Cluster[i])\n",
    "    score = recommend_by_Cluster[i]\n",
    "    networks[i].fit(vRv, score)\n",
    "print(len(networks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Testing\n",
    "Predicted = []\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "for x in range(15000,17721):\n",
    "    vec = vectorizer.transform([cReviews[x]])\n",
    "    prediction = networks[cCluster[x]].predict(vec)[0]\n",
    "    \n",
    "    if prediction == 1:\n",
    "        if cRecommend[x] == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    elif prediction == 0:\n",
    "        if cRecommend[x] == 0:\n",
    "            tn += 1\n",
    "        else: \n",
    "            fn += 1\n",
    "\n",
    "print(\"Overall acc:\" + str((tp+tn) / (tp+tn+fp+fn)))\n",
    "print(\"tp: \" + str(tp) + \" fn: \" + str(fn))\n",
    "print(\"fp: \" + str(fp) + \" tn: \" + str(tn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans with Cosine distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "def cosine_distance (X, Y=None, Y_norm_squared=None, squared=False):\n",
    "    return cosine_distances(X, Y)\n",
    "\n",
    "from sklearn.cluster import k_means_\n",
    "k_means_.euclidean_distances = cosine_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "km.fit(tfidf_reviews)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
